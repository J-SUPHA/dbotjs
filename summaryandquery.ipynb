{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "timestamp_ms = 1713651015101\n",
    "timestamp = timestamp_ms / 1000\n",
    "date_time = datetime.datetime.fromtimestamp(timestamp)\n",
    "print(date_time)  # Output will be a datetime object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"1228090209825063002.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"llama3-70b-8192\", base_url=\"https://api.groq.com/openai/v1\", api_key=\"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\")\n",
    "\n",
    "def query_chroma(query):\n",
    "    docs = db.similarity_search(query)\n",
    "    string = ''\n",
    "    for i in docs:\n",
    "        string += i.page_content\n",
    "    response = model.invoke(f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Your job is to answer the user question using the context.\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Here is the question: {query}\n",
    "Here is the context: {string}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\")\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = query_chroma(\"What other bot name did AusBoss like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "\n",
    "def get_last_x_messages(db_path: str, channel_id: int, k: int, channel_type: str) -> List[Dict]:\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Determine the query based on channel type\n",
    "    if channel_type == \"dm\":\n",
    "        query = \"\"\"\n",
    "        SELECT name, clean_content FROM (\n",
    "            SELECT COALESCE(global_name, user_name) AS name, clean_content, created_timestamp\n",
    "            FROM dms\n",
    "            WHERE channel_id = ?\n",
    "            ORDER BY created_timestamp DESC\n",
    "            LIMIT ?\n",
    "        ) sub ORDER BY created_timestamp ASC\n",
    "        \"\"\"\n",
    "    elif channel_type == \"channel\":\n",
    "        query = \"\"\"\n",
    "        SELECT name, clean_content FROM (\n",
    "            SELECT COALESCE(global_name, user_name) AS name, clean_content, created_timestamp\n",
    "            FROM messages\n",
    "            WHERE channel_id = ?\n",
    "            ORDER BY created_timestamp DESC\n",
    "            LIMIT ?\n",
    "        ) sub ORDER BY created_timestamp ASC\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid channel type\")\n",
    "\n",
    "    # Execute the query and fetch the results\n",
    "    try:\n",
    "        cursor.execute(query, (channel_id, k))\n",
    "        rows = cursor.fetchall()\n",
    "        messages = [{'name': row[0], 'clean_content': row[1]} for row in rows]\n",
    "        message_list = []\n",
    "    \n",
    "        # Iterate over each message in the list\n",
    "        for message in messages:\n",
    "            # Append each message's name and content to the formatted string\n",
    "            message_list.append(f\"{message['name']}: {message['clean_content']}\")\n",
    "    \n",
    "        return message_list\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error fetching messages for {channel_type} with channel ID {channel_id}: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Ensure the database connection is closed after operation\n",
    "        conn.close()\n",
    "\n",
    "# Example usage:\n",
    "db_path = 'messages.db'\n",
    "channel_id=1228090209825063002\n",
    "messages = get_last_x_messages(db_path, channel_id=channel_id, k=999, channel_type='channel')\n",
    "\n",
    "with open(f'{channel_id}.txt', encoding='utf-8', mode='a') as f:\n",
    "    for message in messages:\n",
    "        f.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_x_messages(db_path: str, channel_id: int, k: int, channel_type: str) -> List[Dict]:\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Determine the query based on channel type\n",
    "    if channel_type == \"dm\":\n",
    "        table = \"dms\"\n",
    "    elif channel_type == \"channel\":\n",
    "        table = \"messages\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid channel type\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT user_name AS name, clean_content FROM (\n",
    "        SELECT user_name, clean_content, created_timestamp\n",
    "        FROM {table}\n",
    "        WHERE channel_id = ?\n",
    "        ORDER BY created_timestamp DESC\n",
    "        LIMIT ?\n",
    "    ) sub ORDER BY created_timestamp ASC\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query and fetch the results\n",
    "    cursor.execute(query, (channel_id, k))\n",
    "    rows = cursor.fetchall()\n",
    "    messages = [{'name': row[0], 'clean_content': row[1]} for row in rows]\n",
    "    conn.close()\n",
    "    return messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'messages.db'\n",
    "channel_id = 1228090209825063002\n",
    "messages = get_last_x_messages(db_path, channel_id=channel_id, k=20, channel_type='channel')\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_columns(db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return columns\n",
    "\n",
    "# Example usage to check columns of the 'messages' table\n",
    "columns = get_table_columns('messages.db', 'messages')\n",
    "print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def summarize_groups(messages: List[Dict]):\n",
    "    grouped_messages = [messages[i:i+20] for i in range(0, len(messages), 20)]\n",
    "    summaries = []\n",
    "    \n",
    "    for group in grouped_messages:\n",
    "        context = ' '.join([f\"{msg['name']}: {msg['clean_content']}\" for msg in group])\n",
    "        summary = generate_summary(context)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Assuming you have a function 'generate_summary' that wraps around your model API for summarization\n",
    "# For instance:\n",
    "def generate_summary(text: str) -> str:\n",
    "    response = model.invoke(f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Generate a summary of the conversation and .\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Here is the conversation: {text}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\")\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "db_path = 'messages.db'\n",
    "channel_id = 1228090209825063002\n",
    "messages = get_last_x_messages(db_path, channel_id=channel_id, k=20, channel_type='channel')\n",
    "summaries = summarize_groups(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
